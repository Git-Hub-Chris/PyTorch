"""
This is the top-level configuration module for the compiler, containing
cross-cutting configuration options that affect all parts of the compiler
stack.

You may also be interested in the per-component configuration modules, which
contain configuration options that affect only a specific part of the compiler:

* :mod:`torch._dynamo.config`
* :mod:`torch._inductor.config`
* :mod:`torch._functorch.config`
* :mod:`torch.fx.experimental.config`
"""

import sys
from typing import Optional

# NB: Docblocks go UNDER variable definitions!  Use spacing to make the
# grouping clear.

cache: bool = True
"""When ``True`` (default), enable all (non-experimental) caches supported by
the compiler, persisting compilation products across multiple invocations of
the compiler.  When ``False``, do not enable caches by default (however, if an
individual cache is explicitly enabled, we will respect those settings.)  By
default, caches are written local filesystem at
``/tmp/torchinductor_$USERNAME``.

At time of writing (PyTorch 2.5), the following caches are enabled by default:

* Triton cache, which caches cubins generated by the Triton compiler
  (technically, this cache is implemented by the Triton compiler and not by the
  PyTorch compiler, but this configuration option does control whether or not
  the Triton cache is used.)

* Inductor autotune cache, which caches autotuning results from Inductor
  compilation (this includes tuning for both Inductor generated kernels as
  well as matrix multiply algorithm selection, among others).  This cache is
  controlled by :data:`torch._inductor.config.autotune_local_cache`,
  :data:`torch._inductor.config.autotune_remote_cache`
  (:envvar:`TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE`) and
  :data`torch._inductor.config.bundled_autotune_remote_cache`
  (:envvar:`TORCHINDUCTOR_BUNDLED_AUTOTUNE_REMOTE_CACHE`).

* Inductor FX cache, which caches the Inductor phase of the compiler.
  This cache currently does not subsume the Triton cache, as it only
  caches Python source code that must then be compiled, but as of PyTorch 2.5
  we are in the process of changing this.  This cache is controlled by
  :data:`torch._inductor.config.fx_graph_cache`
  (:envvar:`TORCHINDUCTOR_FX_GRAPH_CACHE`) and
  :data:`torch._inductor.config.fx_graph_remote_cache`
  (:envvar:`TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE`)

The following experimental caches are not currently enabled by default:

* AOTAutograd cache, which caches the AOTAutograd phase of the compiler.  This
  cache subsumes the Inductor FX cache.  This cache is controlled by
  :data:`torch._functorch.config.enable_autograd_cache`
  (:envvar:`TORCHINDUCTOR_AUTOGRAD_CACHE`) and
  :data:`torch._functorch.config.enable_remote_autograd_cache`
  (:envvar:`TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE`).

* Automatic dynamic profile-guided optimization (PGO) cache, which caches
  automatic dynamic determinations for inputs to the graph.  This cache is
  not yet landed to PyTorch nightlies yet.

We reserve the right to change what caches are enabled by default.  Our
contract to you is that caches we enable by default should never introduce a
correctness problem.  You opt-out of the default set of caches by setting
``TORCH_COMPILE_CACHE=0`` (this still alows for individual caches to be
enabled; if you need to guarantee caches are disabled even if user code has
explicitly enabled them by modifying :data:`torch.compiler.config`, use
``TORCH_COMPILE_FORCE_CACHE=0``).

Disabling the cache can be useful if:

* You suspect there to be a bug related to caching, or

* You are modifying PyTorch or its dependencies' source code (we only offer
  best effort cache invalidation when PyTorch is modified), or

* You are trying to instrument the compiler with ``TORCH_LOGS`` or
  ``TORCH_TRACE`` and want to force compilation, rather than observe the
  internal behavior of the compiler on cache hit.

Although we describe this persistent state as a "cache", in some cases we may
exhibit behavior beyond the normal purview of a cache if it allows us to
reduce warmup time for subsequent uses of compiled models.  For example, the
automatic dynamic PGO cache technically changes what ends up being compiled--in
particular, when this cache hits, we will skip compiling the first
encountered static size because we know that you will eventually need the
version of the model that supports dynamic shapes.  An ordinary cache would
more faithfully try to also serve the original static kernel, but we think
this a better choice for the user: the compiler is just-in-time and not even
guaranteed to even try to compile the next thing on the next process
invocation when inputs are varying.

By default, we cache to local filesystem to ``/tmp/torchinductor_$USERNAME``
(this location can be overridden with :envvar:`TORCHINDUCTOR_CACHE_DIR`, to be
renamed soon).  However, we also support a remote cache which can be used to
share caches across multiple nodes, which is useful if, for example, you need
to restart a job on a different set of nodes.  By default, the remote cache is
not enabled, as it requires an external service to host the cache.  Both local
and remote caches can simultaneously be enabled, in which case a local cache
hit always takes precedence over the remote cache.  The remote cache is only
written to in event of a cache miss (e.g., if you hit local cache, this will
*not* refresh TTL of the remote cache.)

"""


from torch.utils._config_module import install_config_module

install_config_module(sys.modules[__name__])
